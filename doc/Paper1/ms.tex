\documentclass[useAMS,usenatbib, a4paper]{mn2e} \usepackage{natbib} 
\addtolength{\voffset}{-0.5in}
\usepackage{graphicx} \usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}

\newcommand{\params}{\boldsymbol{\theta}}
\newcommand{\data}{\boldsymbol{D}}
\newcommand{\info}{\boldsymbol{I}}

%\input{macros.tex}

% -----------------------------------------------------------------------------

\title[Measuring Time Delays]
{A Model for Extracting Time Delays from Noisy Time Series Data} 
\author[B. J. Brewer]{Brendon J. Brewer$^1$, Philip J. Marshall$^2$\\
$^1$Department of Statistics, The University of Auckland, Private Bag 92019,
Auckland 1142, New Zealand\\
$^2$Kavli Institute for Particle Astrophysics and Cosmology, Stanford University, 452 Lomita Mall, Stanford, CA 94035, USA}

\begin{document} 

\date{\today} 
\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2011} 
\maketitle \label{firstpage}

% -----------------------------------------------------------------------------

\begin{abstract}
Observations of gravitationally lensed quasars provide a unique method for
measuring cosmological parameters, particularly the Hubble constant $H_0$.
A key part of
this process is the measurement of the time delays from the photometric
monitoring of the multiple images of the quasar. The uncertainties in the time
delays propagate to the uncertainties about the cosmological parameters, so it
is important to have an accurate understanding of the size of these uncertainties.
Quantifying the
uncertainty in the time delay measurements is challenging, particularly when the time series
data is affected by microlensing and other complex physical effects. We present a
fully probabilistic model for the process of inferring the time delays from
the light curve data. In this model, the probability distribution over light
curves is described by a Gaussian Process (GP) probability distribution whose covariance function
depends on the time delays as well as the properties of the microlensing.
Inferring the parameters of the covariance function from data yields
the time delays, while allowing for a wide range of intrinsic, microlensing, and 
noise-related variability. We test this method on a simple simulated data set
and a challenging one, 
and find the inferences (including parameter uncertainties) to be robust.
We apply our method to radio and optical monitoring data of the well
studied gravitational lens B1608+656, deriving new time delay estimates and uncertainties. The effect on the $H_0$ precision from B1608 is {\bf no idea yet}.
\end{abstract}

\begin{keywords} gravitational lensing --- methods: statistical --- 
\end{keywords}

% -----------------------------------------------------------------------------

\section{Introduction} 

Gravitational lensing has long been recognised as a unique tool for measuring
the cosmological parameters \citep{schechter}. The method makes use of the
intrinsic time variability of a quasar that happens to be multiply imaged. As
the quasar's luminosity varies, each of the multiple images presents the same
fluctuations, but with a time delay $\tau$ that depends on the cosmological
parameters. Therefore, measuring the time delays between the images and
comparing them to those predicted from a model of the gravitational lens
potential allows the cosmological parameters to be estimated.

Historical estimates and problems

The promise of this technique has led to dedicated monitoring programs such as
COSMOGrail \citep[][]{2005A&A...436...25E, 2008A&A...488..481V}. Recently,
thorough investigation of systematic effects and robust lens modelling
techniques have allowed $H_0$ to be measured to a precision of X \% using a
single gravitational lens system \citep{2010ApJ...711..201S}. Specific
systematic effects that were robustly modelled include the uncertainty in the
lens potential reconstruction \citep{2009ApJ...691..277S} and the effects of
external convergence from matter along the line of sight
\citep{2006ApJ...642...30F, 2010ApJ...711..201S}. With large numbers of
suitable gravitational lens systems expected to be found in upcoming surveys
\citep{2010MNRAS.405.2579O}, combining the constraints from many systems can
be expected to yield high precision cosmological measurements
\citep{2010ApJ...712.1378P}, provided sources of systematic error are
understood and well modelled.

However, any inference of $H_0$ or other cosmological parameters
from the gravitational lensing technique inherits
the uncertainty from the measurements of the time delays themselves. A
thorough investigation of all known sources of uncertainty is an integral part
of measuring cosmological parameters, and thus the purpose of this paper is to
develop and present a reliable method for estimating the uncertainty on the
values of the time delays themselves. The method is Bayesian
\citep{2004kats.book.....O} so the results obtained are probability
distributions over the space of possible solutions for the time delays, and the
uncertainties in the time delays
can be summarised by the widths of these probability distributions. To describe
prior knowledge about the underlying time variation of the quasar and the
effects of microlensing on
the light curves, we use Gaussian Processes (GPs), a family of probability
distributions over function space that has computationally convenient
properties \citep{rasmussen}.

Our method for extracting time delays from light curve data is similar in
principle to that presented by \citet{2008ApJ...676...80M}, who described the
effect of microlensing by using simulated microlensing light curves. However,
in practical applications their method becomes computationally prohibitive at
a faster rate than the Gaussian Process approach, because the probability of
finding simulated microlensing tracks that match the observed data decreases
approximately exponentially as the length of the data set grows. In contrast,
GP calculations tend to scale as $O(N^3)$, although this can be improved with
the use of approximations such as conjugate gradient techniques
\citep{gibbsmackay}.

Our method is also related to the curve-shifting approach of
\citet{1996A&A...305...97P}, which aims to find those time delays that
minimise the scatter in the shifted light curves. The primary differences
between the GP approach and standard curve shifting are that the GP approach
is fully Bayesian and allows for exploration of the time delay parameter
space, rather than providing a point estimate of the time delays.
Secondly, it is the likelihood
function, rather than the scatter, which guides the inference, making it clear
how uncertainties are to be obtained. A recent contribution by \citep{2013PhRvD..87l3512H}
described a similar approach to the one presented here. The main differences
are that we implement the model in a Bayesian framework and obtain posterior
samples using MCMC methods, and we also implement several robustness features
(e.g. we allow the possibility of systematically underestimated error bars)
that are not present in the approach of \citet{2013PhRvD..87l3512H}.

Part of our model involves Gaussian Processes, which are a family of probability
distributions over the space of possible functions.
An introduction to Gaussian Processes and their applications can be found in
\citet{rasmussen}. In their applications, GPs are usually used as a choice of
prior distribution over the space of functions. In this work, GPs describe the
sampling distributions, or the probability of the data given the model
parameters. This is a common situation in astrophysics in fields where the
signals are complex and the best models we can think of are 
stochastic processes. Two prominent examples are quasar
variability \citep{2009ApJ...698..895K, 2010ApJ...721.1014M, 2011ApJ...735...80Z}
and solar-like oscillations of stars \citep{2009MNRAS.395.2226B}.

In Section~\ref{sec:bayes} we briefly discuss Bayesian Inference, and in
Section~\ref{sec:sampling} we describe our specific
modelling assumptions. The computational implementation is described in
Section~\ref{sec:computation}. In Section~\ref{sec:tests} we test the inference
with two simulated data sets, one ``easy'' data set and one ``hard'' data set.
In Section~\ref{sec:b1608} we apply the model to B1608+656. Software implementing
the model described in this paper is freely available online at
{\tt www.github.com/eggplantnbren/Flotsam}.

% -----------------------------------------------------------------------------

\section{Bayesian Inference}\label{sec:bayes}
For inferring unknown parameters $\params$ from data $\data$ in the context
of prior information $\info$,
Bayes' rule gives the posterior distribution
\begin{eqnarray}
p(\params | \data, \info) \propto p(\params|\info)p(\data|\params, \info)
\end{eqnarray}
The prior distribution $p(\params|\info)$ describes prior knowledge or uncertainty
about the values of the parameter(s). The ``sampling distributions''
$p(\data|\params,\info)$
describe prior knowledge about the data we expect to observe, as a function of
$\params$ \citep{caticha}. When $\data$ is fixed $p(\data|\params,\info)$ is a function of
$\params$ only and is called the likelihood function.

We begin by defining our model for the prior knowledge about how the time
delays give rise to the observed data. This will determine the sampling
distributions, and also the set of parameters (including nuisance parameters) to
be inferred.

\section{Sampling Distributions}\label{sec:sampling}
The intrinsic quasar variability, in magnitudes, and in whatever band the
observations are carried out in, can be described by a function $q(t)$. For a
two-image system, the resulting light curves, if observed without gaps and
noise, would be described by the functions
\begin{eqnarray}
y_1(t) &=& m_1 + q(t) + \mu_1(t) \\
y_2(t) &=& m_2 + q(t - \tau_2) + \mu_2(t) \\
\end{eqnarray}
where $\{m_1, m_2\}$ are the mean magnitudes of the images,
$\tau_2$ is the time delay of image 2 relative to image 1, and the
functions $\{\mu_1(t), \mu_2(t)\}$ describe any additional fluctuations that
are not shared by both images. A major source of additional fluctuations
is the effect of
microlensing on each of the light curves, which appears because the foreground
lens galaxy is actually composed of stars rather than smoothly
distributed matter \citep{falco}.
The data then consist of
noisy, irregularly sampled measurements of the light curves
$\{y_1(t), y_2(t)\}$.

The actual data we observe is as follows. We will combine the measurements of all
images into a single vector of $N$ magnitude measurements.
At a set of times
$\{t_1, t_2, ..., t_N\}$, we measure the magnitude of one of the images. These
noisy measurements are denoted $\{Y_1, Y_2, ..., Y_N\}$. We also keep track of
a set of flags $\{k_1, k_2, ..., k_N\}, \forall k_i \in \{1, 2\}$ which specifies
which image was measured. For example, if $k_4 = 2$ then we know that $Y_4$ was
a noisy measurement of image two at time $t_4$.

When we use Bayes' rule, the flags $\{k_1, k_2, ..., k_N\}$ and measurement
times $\{t_1, t_2, ..., t_N\}$ will be subsumed into the prior information. The
data (to which we will assign a ``sampling distribution'') will be the
measurements
\begin{eqnarray}
\boldsymbol{D} = \{Y_1, Y_2, ..., Y_N\}.
\end{eqnarray}

The sampling distributions (and hence the likelihood function) will then be
written as a probability distribution for the measurements $\data$
given the time delay $\tau_2$ along with some nuisance parameters. For a full
list of parameters, see Table~\ref{tab:prior}. The total number of parameters
for a two-image system is $13+N$, for a four image system the number of
parameters is $21+N$.




\begin{table*}
\begin{tabular}{lll}
\hline
{\bf Parameter} &	{\bf Meaning}			& {\bf Prior}\\
\hline
$\tau_2$	&	Time delay of image 2	&	Cauchy$\left(0, 0.1\tau_{\rm range}\right)T\left(\tau_{\rm min},\tau_{\rm max}\right)$\\
$\{m_1, m_2\}$	&	Mean magnitude for each image	& Uniform$(m_{\rm min}, m_{\rm max})$\\
$\beta$		&	Size of QSO variations		& LogUniform$(\beta_{\rm min}, \beta_{\rm max})$\\
$L_q$		&	Length scale of QSO variations	& LogUniform$(L_{\rm min}, L_{\rm max})$\\
$\{S_1, S_2\}$	&	Size of image-specific variations	& LogUniform$\left(A_{\rm min}, A_{\rm max}\right)$\\
$\{\ell_1, \ell_2\}$ &	Length scale of image-specific variations & LogUniform$(\ell_{\rm min}, \ell_{\rm max})$\\
$\alpha$	&	Smoothness of image-specific variations	& Uniform$(1, 2)$\\
$f_{\rm bad}$	&	Fraction of ``bad'' data points	& Uniform$(0,1)$\\
$\kappa_1$	&	Error-bar boost for ``good'' data & $\frac{1}{2}\delta(\kappa_1 - 1) + \frac{1}{2}$LogUniform$(1,100)$\\
$\kappa_2$	&	Error-bar boost for ``bad'' data & LogUniform$(\kappa_1, 100\kappa_1)$\\
$\{b_i\}_{i=1}^N$ &	Badness hidden variables for each data point & Uniform$(0,1)$\\
\hline
{\bf Derived Parameter} & {\bf Meaning}	&	{\bf Definition}\\
\hline
$S_{q}$	&	Standard deviation of QSO variability	& $\beta\sqrt{L}$\\
$\{B_i\}_{i=1}^N$	&	Badness flag for each data point & $\mathds{1}\left(b_i \leq f_{\rm bad}\right)$\\
\hline
{\bf Data}	&	{\bf Meaning}			& {\bf Sampling Distribution}\\
\hline
$\{Y_1, ..., Y_N\}$ &	Noisy magnitude measurements	& $\mathcal{N}\left(\mathbf{m}(\params), \mathbf{C}(\params)\right)$\\
\hline
{\bf Prior Information} & {\bf Meaning}			&\\
\hline
$N$		&	Number of measurements (all images together)\\
$\{t_1, ..., t_N\}$ &	Measurement times\\
$\{k_1, ..., k_N\}$ &	Flags identifying which image was observed
\end{tabular}
\caption{A full list of parameters, prior information, and data in the
FLOTSAM model. If there are more than two images, extra parameters are
added accordingly.\label{tab:prior}}
\end{table*}


{\bf GAUSSIAN ERRORS ON MAGNITUDES?}

{\bf IMPORTANCE OF MEAN MAGS FOR MILLILENSING, CORRELATION WITH MICROLENSING AND INTRINSIC AGN TIMESCALE, NEED FOR INFORMATIVE PRIORS}

We model our uncertainties about the underlying QSO light curve $q(t)$ by
a GP with mean function $\mathds{E}\left(q(t)\right) = 0$ and covariance function
\begin{eqnarray}
\textnormal{Cov}\left[q(t_1), q(t_2)\right] = S_q\exp
\left[-\frac{|t_2 - t_1|}{L_q}\right]
\end{eqnarray}

This model for QSO variability has been studied extensively by \citep{}
{\bf COMMENT ON KELLY}. We also note that this distribution is a MaxEnt
distribution \citep{} with XX constraints \citep{sivia}.

Similarly, our uncertainty about the image-specific variability is modelled
as a GP with mean function $\mathds{E}\left(\mu_i(t)\right) = 0$ and
covariance function
\begin{eqnarray}
\textnormal{Cov}\left[\mu_i(t_1), \mu_i(t_2)\right] =
S_i\exp\left[
-\left(\frac{|t_2 - t_1|
}{\ell_i}\right)^\alpha
\right]
\end{eqnarray}

There is a separate $S_i$ parameter $\ell_i$ parameter
for each image; this is because the stellar density and lens convergence is
different at each image position. If the image-specific variability is
predominantly caused by microlensing, the typical timescales and amplitudes
of the microlensing are unlikely to be the same for each image.
In contrast, we use a single $\alpha$ parameter for all images;
this is because it is suspected that the
smoothness of the microlensing variations, which $\alpha$ describes, depends
primarily on the size of the microlensed source, and all images are of the
same source. We also assume
prior independence of $q(t)$, $\mu_1(t)$, and $\mu_2(t)$.

See the Appendix for derivations.


{RANT ABOUT HOW THE GP ASSUMPTION FOR "MICROLENSING" IS A DESCRIPTION OF OUR
PRIOR UNCERTAINTY ABOUT THE SIGNAL **IN OUR PARTICULAR DATA SET**}

The probability distribution for the observed data $\mathbf{y}$ given the
model parameters $\boldsymbol{\theta}$ is a multivariate Gaussian:
\begin{equation}
p(\mathbf{y} | \theta) = \frac{1}{\sqrt{(2\pi)^n \det \mathbf{C}}}
\exp\left[-\frac{1}{2}
(\mathbf{y} - \mathbf{m})^T\mathbf{C}^{-1}(\mathbf{y} - \mathbf{m})\right]
\end{equation}
where $\mathbf{m}$ is the vector of mean values, and $\mathbf{C}$ is the
covariance matrix of the data, formed by evaluating the covariance function of
the GP at the times of the observations, and then adding diagonal components
to describe the observational errors. That is,
\begin{equation}
\mathbf{C} = \textnormal{Cov}\left[y_i(t_1), y_j(t_2)\right] + \mathbf{C}_{\rm noise}.
\end{equation}
{\bf TODO: WRITE THIS PROPERLY}

Note that in this formalism we are using a  GP to describe the {\it
uncertainty} about the microlensing contributions to the light curves so that
we can model the data. This is not quite the same as asserting that the
microlensing magnifications were generated by a Gaussian Process; it is a way
of accoutning for microlensing fluctuations when inferring the time delays. It
is also the case that {\it other} sources of light curve fluctuation, such as
outliers caused by single epoch calibration errors, will be absorbed by the
``microlensing'' process -- the robustness of the time delay inference in that
situation is something we test in Section~\ref{simdata}. It would be more
realistic, yet much more computationally demanding, to use actual simulated
microlensing  light curves \citep{1999JCoAM.109..353W, 2008ApJ...676...80M,
2010NewA...15..181G}. However, for large source sizes or far from caustic
crossing events, we might expect that the GP approximation is adequate. In
Section~\ref{simdata} we test the model on simulated data containing realistic
microlensing effects, and demonstrate that the time delays are still reliably
inferred despite the GP assumption.

A common situation in astronomy is that final data products, such as light
curves, are returned, with error bars, from a pipeline. However, such error
bars are not always trustworthy, as they may have been underestimated due to
modelling assumptions and approximations in the pipeline. In the context of
light curve modelling, underestimated error bars can be detected if the
scatter in the light curve is better modelled by independent Gaussian noise
than it is by a time-correlated microlensing term. Thus, to improve the
robustness of our method, we include a ``noise boost'' parameter $\kappa$
describing the factor by which the reported error bars should be multiplied.
This is another mechanism for accounting for outliers.

In summary, the parameter vector is
\begin{equation}
\left\{ \{\tau\}, \{m\}, \sigma_{\rm QSO}, L_{\rm QSO}, \{\sigma_{\rm micro}\}
\{L_{\rm micro}\}, \alpha_{\rm micro}, \kappa \right\}
\end{equation}
where parameters in braces denote a set (one for each QSO image).  For a
double image lens, the total number of parameters in the model is~12. For a
quad, this number is~20. A set of uninformative prior
distributions for these parameters are listed in Table~\ref{priors}; we assign
these priors in the analysis described in the first part of this paper. Later,
we will show how a more informative set of priors can be derived, and used to
improve the precision of the time delay inferences {\bf (WE HOPE)}.


\subsection{Computational Details}

We use Diffusive Nested Sampling \citep{dnest} to explore the posterior
distribution for the parameters. Metropolis-Hastings updates are used for all
parameters. For those that do not affect the covariance matrix (the $m$'s),
the Cholesky decomposition is not recomputed, making these steps fast. {\bf
DESCRIBE CONSTRUCTION OF CHOLESKY DECOMP AND MOTIVATION - CITE KOSLOWSKI,
R\&W}


% -----------------------------------------------------------------------------

\section{Tests on Simulated Data}\label{simdata}

In this section, we demonstrate the method on simulated data. We first show
that, in the absence of microlensing, we can recover the input time delays and
GP parameters of the intrinsic AGN lightcurve. We then go on to test the
method on the same mock lens systems, but now with plausible microlensing
fluctuations added. These data violate the modelling assumptions, particularly
that of the probability distribution for the microlensing variations being a
GP. 


\subsection{Intrinsic AGN Variability}

In this section we describe our simulated AGN lightcurves in more detail, and
then present the results of the GP inference, all in the absence of any
microlensing. Can we recover the time delays, in all observing strategies? Can
we simultaneously recover the input variability parameters?

\subsubsection{Methodology}

We use the damped random walk, or CAR(1) model of \citet{Kel++09} to simulate
the optical variability of lensed quasars. This is a stochastic model: the
statistics of lightcurves generated by the CAR(1) process are fuly described
by three parameters: the mean magnitude, the decay timescale $\tau_q$ and the
amplitude of  variability $\sigma_q$. While \citeauthor{Kel++09} describe an
efficient processs for generating simulated lightcurves,  we note that the
CAR(1) model is a Gaussian Process with exponential covariance function
\citep{Zu++10}: the CAR(1) parameters are exactly equivalent to those of our
model (Section~\ref{sec:model}). 

We choose three representative values of $\tau_q$ and $\sigma_q$ to span the
space of possibilities for lensed quasars (Table~\ref{tab:simpars}).  These
were estimated from the  distributions for the Stripe 82 SDSS spectroscopic
quasar sample of \citet{Kel++09}. We note that the possible timescales range
over two orders of magnitude, making the distinction between observed frame
and rest-frame negligible. When simulating lightcurves for each image, we make
some assumptions to simplify the interpretation of our results. We only
consider measurements in the $i$-band, and we choose the mean magnitude of the
unlensed source quasar to be 23.5 in each case, such that the faintest image
in any given quad lens system lies at or above the 5-sigma $i$-band detection
limit \citep{Ive++10}. Indeed, we assume constant image quality and depth at
every observation epoch, resulting in a constant assumed 5-sigma AB limiting
magnitude per image of 24. 
The different magnifications of the images then lead
naturally to a range of different monitoring signal to noise ratios.
Fluxes corresponding to the lensed image magnitudes
at each epoch were computed, and Gaussian background sky noise added such that a 24th
magnitude point source lay at 5-sigma above zero flux. We ignore readnoise and
shot noise
from the images themselves, an approximation confirmed to be good to a few
percent using the LSST exposure time calculator\footnote{LSST ETC url here.}
with fiducial values of the
sky brightnessss. 
The resulting noisy
fluxes were then converted back to magnitudes. This procedure, while
approximate, captures the fact that it is the image fluxes that have Gaussian
distributed uncertainties: in our inference we assume Gaussian likelihoods for
the magnitudes, an assumption that we need to test. We provide an uncertainty
estimate for each magnitude consistent with these assumptions, based on
propagating the assumed errors on the flux to an uncertainty in the
corresponding magnitude.


\subsection{Including Microlensing}

In this section we describe our simulations of the microlensing occurring at
each image position, and then repeat the inferences of the previous section in
the presence of this contaminant. How is the time delay inference degraded?
Can the microlensing curves be modelled by the GP? How are the results
affected by observing strategy?


\subsubsection{Methodology}
\subsection{Discussion}

What did we learn from simulated data?
When do we succeed and when do we fail?
WHat future work is required?

% -----------------------------------------------------------------------------

\section{Application to B1608$+$656}

B1608$+$656 is a gravitational lens system a radio-loud source galaxy; it has 
very precisely measured time delays, and has been successfully used as a
cosmographic probe \citep[e.g.][]{Suy++10}.  Three seasons of VLA monitoring
were taken by  \citet{2002ApJ...581..823F, 1999ApJ...527..498F},  and are
shown in Figure~\ref{b1608_data}. The dominant effect visible in the data is
the gradual change in the flux of the background AGN over time, visible in all
four images. This gradual change is not so useful for measuring the time
delays. Instead, the time delay information comes mostly from small short-term
fluctuations on top of the global trend. The size of the radio source is
assumed to be much larger than the Einstein radius of the stars in the lens
galaxy, leading to negligible microlensing. In this section we model the
B1608$+$656 radio lightcurve data with our combined intrinsic plus
microlensing Gaussian process, to test this assumption and re-measure the
system time delays.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}
%\includegraphics[scale=0.75]{figs1/b1608_data.eps}
%\caption{Monitoring data for B1608+656.\label{b1608_data}}
%\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}
%\includegraphics{figs1/b1608_delays.eps}
%\caption{Inferred time delays for B1608+656.\label{b1608_delays}}
%\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}
%\includegraphics[scale=0.42]{figs1/correlation.eps}
%\caption{Inferred intrinsic QSO variability parameters for B1608+656. This is the only significant correlation in the joint posterior distribution. The data are consistent with a small or large value for the variability amplitude, but if the amplitude is large, then the timescale must also be large so that we have only observed a small portion of the timescale.\label{correlation}}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% -----------------------------------------------------------------------------

\section{Application to J1131}

\citep{2006astro.ph..5321M}


% -----------------------------------------------------------------------------

\section{Discussion and Further Work}

Robustness of time delays.

Effect of outliers. Do we deal with them well enough in th ecurrent implementation?

Mean mags. implications for millilensing.

Microlensing pars, good enough? Future: inference of physical microlensing parameters. 
Use of informative priors.

Multi-wavelength extension - colour variability.

GP approximations for speed up in LSST era.


% -----------------------------------------------------------------------------

\section{Conclusions}

\begin{itemize}
\item Microlensing can be approximated as GP
\item Time delays and covariance between them can be robustly estimated
\item B1608
\item J1131
\end{itemize}


% -----------------------------------------------------------------------------

\section{Acknowledgements}
David Hogg, Jo Bovy, Malte Tewes, Matt Auger, Sherry Suyu, Chris Fassnacht, Tommaso Treu, Alicia Berciano Alba
(some may be upgraded to authors)

% -----------------------------------------------------------------------------

\begin{thebibliography}{99} 
\bibitem[\protect\citeauthoryear{Brewer 
\& Stello}{2009}]{2009MNRAS.395.2226B} Brewer B.~J., Stello D., 2009, MNRAS, 395, 2226

\bibitem[{Brewer {et~al.}(2010)Brewer, P\'{a}rtay, \& Cs\'{a}nyi}]{dnest}
Brewer, B., P\'{a}rtay, L., \& Cs\'{a}nyi, G. 2010, Statistics and Computing, doi: 10.1007/s11222-010-9198-8, arXiv: 0912.2380

\bibitem[\protect\citeauthoryear{Caticha}{2008}]{caticha} 
Caticha A., 2008, arXiv, arXiv:0808.0012 

\bibitem[\protect\citeauthoryear{Eigenbrod et 
al.}{2005}]{2005A&A...436...25E} Eigenbrod A., Courbin F., Vuissoz C., Meylan G., Saha P., Dye S., 2005, A\&A, 436, 25 

\bibitem[\protect\citeauthoryear{Falco, Wambsganss, 
\& Schneider}{1991}]{falco} Falco E.~E., Wambsganss J., Schneider P., 1991, MNRAS, 251, 698 

\bibitem[\protect\citeauthoryear{Fassnacht et 
al.}{2006}]{2006ApJ...642...30F} Fassnacht C.~D., Gal R.~R., Lubin L.~M., 
McKean J.~P., Squires G.~K., Readhead A.~C.~S., 2006, ApJ, 642, 30 

\bibitem[\protect\citeauthoryear{Fassnacht et 
al.}{2002}]{2002ApJ...581..823F} Fassnacht C.~D., Xanthopoulos E., Koopmans 
L.~V.~E., Rusin D., 2002, ApJ, 581, 823 

\bibitem[\protect\citeauthoryear{Fassnacht et 
al.}{1999}]{1999ApJ...527..498F} Fassnacht C.~D., Pearson T.~J., Readhead 
A.~C.~S., Browne I.~W.~A., Koopmans L.~V.~E., Myers S.~T., Wilkinson P.~N., 
1999, ApJ, 527, 498 

\bibitem[\protect\citeauthoryear{Garsden 
\& Lewis}{2010}]{2010NewA...15..181G} Garsden H., Lewis G.~F., 2010, NewA, 15, 181 

\bibitem[\protect\citeauthoryear{Gibbs \& MacKay}{1997}]{gibbsmackay}
Gibbs, M., Mackay, D., 1997, Efficient implementation of Gaussian processes, Cavendish Lab., Cambridge, U.K., Tech. Rep.

\bibitem[\protect\citeauthoryear{Hojjati, Kim, 
\& Linder}{2013}]{2013PhRvD..87l3512H} Hojjati A., Kim A.~G., Linder E.~V., 2013, PhRvD, 87, 123512 

\bibitem[\protect\citeauthoryear{Kelly, Bechtold, 
\& Siemiginowska}{2009}]{2009ApJ...698..895K} Kelly B.~C., Bechtold J., Siemiginowska A., 2009, ApJ, 698, 895 

\bibitem[\protect\citeauthoryear{MacLeod et 
al.}{2010}]{2010ApJ...721.1014M} MacLeod C.~L., et al., 2010, ApJ, 721, 
1014 

\bibitem[\protect\citeauthoryear{Morgan et al.}{2008}]{2008ApJ...676...80M} 
Morgan C.~W., Eyler M.~E., Kochanek C.~S., Morgan N.~D., Falco E.~E., 
Vuissoz C., Courbin F., Meylan G., 2008, ApJ, 676, 80 

\bibitem[\protect\citeauthoryear{Morgan et al.}{2006}]{2006astro.ph..5321M} 
Morgan N.~D., Kochanek C.~S., Falco E.~E., Dai X., 2006, astro, 
arXiv:astro-ph/0605321 

\bibitem[\protect\citeauthoryear{Oguri 
\& Marshall}{2010}]{2010MNRAS.405.2579O} Oguri M., Marshall P.~J., 2010, MNRAS, 405, 2579 

\bibitem[\protect\citeauthoryear{O'Hagan 
\& Forster}{2004}]{2004kats.book.....O} O'Hagan A., Forster J., 2004, Kendall's Advanced Theory of Statistics, Volume 2B: Bayesian Inference, London: Hodder Arnold, 2004

\bibitem[\protect\citeauthoryear{Paraficz 
\& Hjorth}{2010}]{2010ApJ...712.1378P} Paraficz D., Hjorth J., 2010, ApJ, 712, 1378 

\bibitem[\protect\citeauthoryear{Pelt et 
al.}{1996}]{1996A&A...305...97P} Pelt J., Kayser R., Refsdal S., Schramm T., 1996, A\&A, 305, 97 

\bibitem[\protect\citeauthoryear{Rasmussen \& Williams}{2006}]{rasmussen} Rasmussen C., Williams C., Gaussian Processes for Machine Learning, The MIT Press, 2006

\bibitem[\protect\citeauthoryear{Schechter}{2005}]{schechter} 
Schechter P.~L., 2005, IAUS, 225, 281 

\bibitem[\protect\citeauthoryear{Suyu et al.}{2010}]{2010ApJ...711..201S} 
Suyu S.~H., Marshall P.~J., Auger M.~W., Hilbert S., Blandford R.~D., 
Koopmans L.~V.~E., Fassnacht C.~D., Treu T., 2010, ApJ, 711, 201 

\bibitem[\protect\citeauthoryear{Suyu et al.}{2009}]{2009ApJ...691..277S} 
Suyu S.~H., Marshall P.~J., Blandford R.~D., Fassnacht C.~D., Koopmans 
L.~V.~E., McKean J.~P., Treu T., 2009, ApJ, 691, 277 

\bibitem[\protect\citeauthoryear{Vuissoz et 
al.}{2008}]{2008A&A...488..481V} Vuissoz C., et al., 2008, A\&A, 488, 481 

\bibitem[\protect\citeauthoryear{Wambsganss}{1999}]{1999JCoAM.109..353W} 
Wambsganss J., 1999, JCoAM, 109, 353 

\bibitem[\protect\citeauthoryear{Zu, Kochanek, 
\& Peterson}{2011}]{2011ApJ...735...80Z} Zu Y., Kochanek C.~S., Peterson B.~M., 2011, ApJ, 735, 80 
\end{thebibliography}

\appendix
\onecolumn

\section{GP Covariance Function}
Recall that the noise-free light curves of the two images are given by:
\begin{eqnarray}
y_1(t) &=& m_1 + q(t - \tau_1) + \mu_1(t) \\
y_2(t) &=& m_2 + q(t - \tau_2) + \mu_2(t)
\end{eqnarray}
where $\tau_1 = 0$.
Using this, and the model assumptions discussed in the paper, we can obtain the
covariance of image $i$ at time $t_1$ with image $j$ at time $t_2$, where
$i \neq j$:
\begin{eqnarray}
\textnormal{Cov}\left[y_i(t_1), y_j(t_2)\right]
&=& \mathds{E}\left[\left(y_i(t_1) - m_i\right)\left(y_j(t_2) - m_j\right)\right]\\
&=& \mathds{E}\left[\left(q(t_1 - \tau_i) + \mu_i(t_1)\right)
\left(q(t_2 - \tau_j) + \mu_j(t_2)\right)\right]\\
&=& \mathds{E}\left[q(t_1 - \tau_i)q(t_2 - \tau_j) + q(t_1 - \tau_i)\mu_j(t_2)
+ \mu_i(t_1)q(t_2 - \tau_j) + \mu_i(t_1)\mu_j(t_2)
\right]\\
&=& S_q\exp
\left[-\frac{|(t_2 -\tau_j)- (t_1-\tau_i)|}{L_q}\right].
\end{eqnarray}
We can also obtain the covariance of image $i$ measured at two times $t_1$ and
$t_2$:
\begin{eqnarray}
\textnormal{Cov}\left[y_i(t_1), y_i(t_2)\right]
&=& \mathds{E}\left[\left(y_i(t_1) - m_i\right)\left(y_i(t_2) - m_i\right)\right]\\
&=& \mathds{E}\left[\left(q(t_1 - \tau_i) + \mu_i(t_1)\right)
\left(q(t_2 - \tau_i) + \mu_i(t_2)\right)\right]\\
&=& \mathds{E}\left[q(t_1 - \tau_i)q(t_2 - \tau_i) + q(t_1 - \tau_i)\mu_i(t_2)
+ \mu_i(t_1)q(t_2 - \tau_i) + \mu_i(t_1)\mu_i(t_2)
\right]\\
&=& S_q\exp
\left[-\frac{|(t_2 -\tau_j)- (t_1-\tau_i)|}{L_q}\right] +
S_i\exp\left[
-\left(\frac{|t_2 - t_1|
}{\ell_i}\right)^\alpha
\right]
.
\end{eqnarray}
The covariance between a data point $Y_i$ and another data point $Y_j$ is
given by the covariance of the underlying noise-free light curves at times $t_i$
and $t_j$, plus a noise term:
\begin{eqnarray}
\textnormal{Cov}\left(
Y_i, Y_j
\right) = \textnormal{Cov}\left(y_{k_i}(t_i), y_{k_j}(t_j)\right)
+ \delta_{ij}\left[\left[\kappa_1 + (\kappa_2 - \kappa_1)\mathds{1}(B_i = 1)\right]\sigma_i\right]^2
\end{eqnarray}
where $\sigma_i$ is the reported error bar given with the data. The coefficient
in front of $\sigma_i$ implies that the error bar should be multiplied by
$\kappa_1$ if this is a ``good'' data point, or by $\kappa_2$ if it is a ``bad''
data point.


\end{document}

